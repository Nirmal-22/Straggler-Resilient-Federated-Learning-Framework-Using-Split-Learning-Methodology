{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3967f5f-9e4c-4526-a5c7-e13601ef61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import socket\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from threading import Thread\n",
    "from threading import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57caf1f4-e503-4f84-bc7f-cead35226aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = 2\n",
    "user_ram = [8,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff63f2ff-b701-4a29-b5a2-d8c823b7b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 10\n",
    "local_epoch = 1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c78f7-0985-41a0-a5f6-b449fe725f2c",
   "metadata": {},
   "source": [
    "## CUDA Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31c166b-96c2-4814-8628-68999a68a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de0076-b39a-4bdb-a289-fca2ef2c19f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03bdfa3-8f25-4132-8992-2724d8aa95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hyperparameter_federated(userlist):\n",
    "#     clientsoclist = [0]*userlist\n",
    "    \n",
    "#     start_time = 0\n",
    "#     weight_count = 0\n",
    "    \n",
    "#     global_weights = copy.deepcopy(ecg_net.state_dict())\n",
    "    \n",
    "#     datasetsize = [0]*userlist\n",
    "#     weights_list = [0]*userlist\n",
    "\n",
    "#     return clientsoclist,start_time,weight_count,global_weights,datasetsize,weights_list\n",
    "# ## if user > 3 then put one user in split user ram = [10,8,4]\n",
    "\n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0709050d-b387-4c9b-93ac-2ac8c64091d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sendsize_list = []\n",
    "total_receivesize_list = []\n",
    "\n",
    "client_sendsize_list = [[] for i in range(users)]\n",
    "client_receivesize_list = [[] for i in range(users)]\n",
    "\n",
    "train_sendsize_list = [] \n",
    "train_receivesize_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3ca176-242f-4efa-9a8a-40a2095e91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hyperparameter_split():\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     lr = 0.001\n",
    "#     optimizer_server = Adam(ecg_net_splitserver.parameters(), lr=lr)\n",
    "    \n",
    "#     clientsoclist = []\n",
    "#     train_total_batch = []\n",
    "#     val_acc = []\n",
    "#     client_weights = copy.deepcopy(ecg_net_splitclient.state_dict())\n",
    "    \n",
    "#     train_acc = []\n",
    "#     val_acc = []\n",
    "#     return criterion,lr,optimizer_server,clientsoclist,train_total_batch, val_acc,client_weights,train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc6295-7e86-4830-a074-842ed29fe254",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb667c0-7869-4af9-aafd-98f8c8c02a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1523f14f-d874-4bf8-9a40-762e77bb0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used only in split learning\n",
    "class ECG(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            with h5py.File(os.path.join(root_path, 'ecg_data', 'train_ecg.hdf5'), 'r') as hdf:\n",
    "                self.x = hdf['x_train'][:]\n",
    "                self.y = hdf['y_train'][:]\n",
    "        else:\n",
    "            with h5py.File(os.path.join(root_path, 'ecg_data', 'test_ecg.hdf5'), 'r') as hdf:\n",
    "                self.x = hdf['x_test'][:]\n",
    "                self.y = hdf['y_test'][:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx], dtype=torch.float), torch.tensor(self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f55c3fc3-e4e2-4275-ac87-d38d035efa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66e3f80-e0a7-4fbd-8b15-6f49bd79ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ECG(train=True)\n",
    "test_dataset = ECG(train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b4869-d389-4dcd-931b-7cff326b96c1",
   "metadata": {},
   "source": [
    "## Construct 1D-CNN ECG classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02f97de5-a3e9-4a49-a2cc-93c9f1fb71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcgConv1d_Federated(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EcgConv1d_Federated, self).__init__()        \n",
    "        self.conv1 = nn.Conv1d(1, 16, 7)  # 124 x 16        \n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)  # 62 x 16\n",
    "        self.conv2 = nn.Conv1d(16, 16, 5)  # 58 x 16\n",
    "        self.relu2 = nn.LeakyReLU()        \n",
    "        self.conv3 = nn.Conv1d(16, 16, 5)  # 54 x 16\n",
    "        self.relu3 = nn.LeakyReLU()        \n",
    "        self.conv4 = nn.Conv1d(16, 16, 5)  # 50 x 16\n",
    "        self.relu4 = nn.LeakyReLU()\n",
    "        self.pool4 = nn.MaxPool1d(2)  # 25 x 16\n",
    "        self.linear5 = nn.Linear(25 * 16, 128)\n",
    "        self.relu5 = nn.LeakyReLU()        \n",
    "        self.linear6 = nn.Linear(128, 5)\n",
    "        self.softmax6 = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        x = x.view(-1, 25 * 16)\n",
    "        x = self.linear5(x)\n",
    "        x = self.relu5(x)        \n",
    "        x = self.linear6(x)\n",
    "        x = self.softmax6(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ad8272-e04e-4eb9-9416-396b15a2e79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EcgConv1d_Federated(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(7,), stride=(1,))\n",
       "  (relu1): LeakyReLU(negative_slope=0.01)\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n",
       "  (relu2): LeakyReLU(negative_slope=0.01)\n",
       "  (conv3): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n",
       "  (relu3): LeakyReLU(negative_slope=0.01)\n",
       "  (conv4): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n",
       "  (relu4): LeakyReLU(negative_slope=0.01)\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear5): Linear(in_features=400, out_features=128, bias=True)\n",
       "  (relu5): LeakyReLU(negative_slope=0.01)\n",
       "  (linear6): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (softmax6): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg_net_Federated = EcgConv1d_Federated()\n",
    "ecg_net_Federated.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10edf0b-1708-48d1-baab-3ad602ef7cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27727424-613c-4eed-bdcb-9125927095fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcgConv1d_SplitServer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EcgConv1d_SplitServer, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(1, 16, 7, padding=3)  # 128 x 16\n",
    "#         self.relu1 = nn.LeakyReLU()\n",
    "#         self.pool1 = nn.MaxPool1d(2)  # 64 x 16\n",
    "#         self.conv2 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "#         self.relu2 = nn.LeakyReLU()\n",
    "        self.conv3 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.conv4 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "        self.relu4 = nn.LeakyReLU()\n",
    "        self.conv5 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "        self.relu5 = nn.LeakyReLU()\n",
    "        self.pool5 = nn.MaxPool1d(2)  # 32 x 16\n",
    "        self.linear6 = nn.Linear(32 * 16, 128)\n",
    "        self.relu6 = nn.LeakyReLU()\n",
    "        self.linear7 = nn.Linear(128, 5)\n",
    "        self.softmax7 = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.pool5(x)\n",
    "        x = x.view(-1, 32 * 16)\n",
    "        x = self.linear6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.linear7(x)\n",
    "        x = self.softmax7(x)\n",
    "        return x       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98540c04-6fea-412d-803e-bbb580171d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EcgConv1d_SplitServer(\n",
      "  (conv3): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu3): LeakyReLU(negative_slope=0.01)\n",
      "  (conv4): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu4): LeakyReLU(negative_slope=0.01)\n",
      "  (conv5): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu5): LeakyReLU(negative_slope=0.01)\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear6): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (relu6): LeakyReLU(negative_slope=0.01)\n",
      "  (linear7): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (softmax7): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ecg_net_splitserver = EcgConv1d_SplitServer().to(device)\n",
    "print(ecg_net_splitserver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af2c01e-dd23-41ee-a98b-72e9d5fddb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcgConv1d_SplitClient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EcgConv1d_SplitClient, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, 7, padding=3)  # 128 x 16\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)  # 64 x 16\n",
    "        self.conv2 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20048c9e-695f-4873-9fd5-ecb8c9ccd725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EcgConv1d_SplitClient(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (relu1): LeakyReLU(negative_slope=0.01)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu2): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ecg_net_splitclient = EcgConv1d_SplitClient().to('cpu')\n",
    "print(ecg_net_splitclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73839ada-a067-438f-bd51-aa0ae482cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# lr = 0.001\n",
    "# optimizer_splitserver = Adam(ecg_net_splitserver.parameters(), lr=lr)\n",
    "# optimizer_Federated = Adam(ecg_net_Federated.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaabf12-d934-42f2-9dab-fde7e703a634",
   "metadata": {},
   "source": [
    "## Socket Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d5b5735-07a3-40f8-99ef-53bc38ec5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msglen\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a03d700d-c574-4ab5-8fba-2e5f70d2a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeweights(dict1,dict2):\n",
    "    for i in dict2.keys():\n",
    "        dict1[i]=dict2[i]\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3533e67d-61ca-468a-b7ae-aac36942fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer_server = Adam(ecg_net_splitserver.parameters(), lr=lr)\n",
    "\n",
    "# clientsoclist = []\n",
    "train_total_batch = []\n",
    "val_acc = []\n",
    "client_weights_split = copy.deepcopy(ecg_net_splitclient.state_dict())\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e447c7-d544-4593-93a6-2836bc7c7af6",
   "metadata": {},
   "source": [
    "# Split Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4ffe793-37a1-4a8b-bfe0-055771dc9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = {}\n",
    "\n",
    "# def train_split(split_users,criterion,lr,optimizer_server,clientsoclist,train_total_batch, val_acc,client_weights,train_acc):\n",
    "def train_split(userid, num_users, conn,client_weights):\n",
    "\n",
    "    # criterion,lr,optimizer_server,clientsoclist,train_total_batch, val_acc,client_weights,train_acc = hyperparameter_split()\n",
    "    # conn, addr = s.accept()\n",
    "    # print('Connected with', addr)\n",
    "    # clientsoclist.append(conn)    # append client socket on list\n",
    "    i = userid\n",
    "    datasize = send_msg(conn, epochs)    #send epoch\n",
    "    total_sendsize_list.append(datasize)\n",
    "    client_sendsize_list[i].append(datasize)\n",
    "\n",
    "    total_batch, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    "    total_receivesize_list.append(datasize)\n",
    "    client_receivesize_list[i].append(datasize)\n",
    "\n",
    "    train_total_batch.append(total_batch)    # append on list\n",
    "    start_time = time.time()    # store start time\n",
    "    split_users = 1\n",
    "    print(\"Timer start!\")\n",
    "\n",
    "\n",
    "    global total_weights\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        # train client 0\n",
    "        client_weights_out = client_weights\n",
    "        for user in range(split_users):\n",
    "    \n",
    "            datasize = send_msg(clientsoclist_split[user], client_weights)\n",
    "            total_sendsize_list.append(datasize)\n",
    "            client_sendsize_list[user].append(datasize)\n",
    "            train_sendsize_list.append(datasize)\n",
    "    \n",
    "            for i in tqdm(range(train_total_batch[user]), ncols=100, desc='Epoch {} Client{} '.format(e+1, user)):\n",
    "                optimizer_server.zero_grad()  # initialize all gradients to zero\n",
    "    \n",
    "                msg, datasize = recv_msg(clientsoclist_split[user])  # receive client message from socket\n",
    "                total_receivesize_list.append(datasize)\n",
    "                client_receivesize_list[user].append(datasize)\n",
    "                train_receivesize_list.append(datasize)\n",
    "    \n",
    "                client_output_cpu = msg['client_output']  # client output tensor\n",
    "                label = msg['label']  # label\n",
    "    \n",
    "                client_output = client_output_cpu.to(device)\n",
    "                # print(\"Client output for user\",user,\"output:\",client_output_cpu[0])\n",
    "                label = label.clone().detach().long().to(device)\n",
    "    \n",
    "                output = ecg_net_splitserver(client_output)  # forward propagation\n",
    "                # print(\"Server output for user\",user,\"output:\",output[0])\n",
    "                loss = criterion(output, label)  # calculates cross-entropy loss\n",
    "    \n",
    "                # loss_values.append(loss.item())\n",
    "                # accuracy = calculate_accuracy(output, label)\n",
    "                # accuracy_values.append(accuracy)\n",
    "                \n",
    "                # cpu_percent, memory_percent = get_cpu_memory_usage()\n",
    "                # cpu_values.append(cpu_percent)\n",
    "                \n",
    "                # memory_values.append(memory_percent)\n",
    "                \n",
    "                # elapsed_time = time.time() - start_time\n",
    "                # time_values.append(elapsed_time)\n",
    "                \n",
    "                loss.backward()  # backward propagation\n",
    "                msg = client_output_cpu.grad.clone().detach()\n",
    "    \n",
    "                datasize = send_msg(clientsoclist_split[user], msg)\n",
    "                total_sendsize_list.append(datasize)\n",
    "                client_sendsize_list[user].append(datasize)\n",
    "                train_sendsize_list.append(datasize)\n",
    "                \n",
    "                optimizer_server.step()\n",
    "                \n",
    "            client_weights, datasize = recv_msg(clientsoclist_split[user])\n",
    "            total_receivesize_list.append(datasize)\n",
    "            client_receivesize_list[user].append(datasize)\n",
    "            train_receivesize_list.append(datasize)\n",
    "\n",
    "            client_weights_out = client_weights\n",
    "            client_weights_copy = client_weights\n",
    "\n",
    "            server_weights = ecg_net_splitclient.state_dict()\n",
    "            split_weights = mergeweights(server_weights,client_weights_copy)\n",
    "\n",
    "        \n",
    "        ecg_net_splitclient.load_state_dict(client_weights_out)\n",
    "        ecg_net_splitclient.to(device)\n",
    "        ecg_net_splitclient.eval()\n",
    "    \n",
    "    \n",
    "        # train acc\n",
    "        with torch.no_grad():\n",
    "            corr_num = 0\n",
    "            total_num = 0\n",
    "            train_loss = 0.0\n",
    "            for j, trn in enumerate(train_loader):\n",
    "                trn_x, trn_label = trn\n",
    "                trn_x = trn_x.to(device)\n",
    "                trn_label = trn_label.to(device)\n",
    "    \n",
    "                trn_output = ecg_net_splitclient(trn_x)\n",
    "                # print(\"Client side output for user \",user,\" out:\",trn_output[0])\n",
    "                trn_output = ecg_net_splitserver(trn_output)\n",
    "                # print(\"Server side output for user \",user,\" out:\",trn_output[0])\n",
    "                trn_label = trn_label.long()\n",
    "                loss = criterion(trn_output, trn_label)\n",
    "                train_loss += loss.item()\n",
    "                model_label = trn_output.argmax(dim=1)\n",
    "                corr = trn_label[trn_label == model_label].size(0)\n",
    "                corr_num += corr\n",
    "                total_num += trn_label.size(0)\n",
    "    \n",
    "            train_accuracy = corr_num / total_num * 100\n",
    "            r_train_loss = train_loss / len(train_loader)\n",
    "            print(\"train_acc: {:.2f}%, train_loss: {:.4f}\".format(train_accuracy, r_train_loss))\n",
    "            train_acc.append(train_accuracy)\n",
    "        \n",
    "        # test acc\n",
    "        with torch.no_grad():\n",
    "            corr_num = 0\n",
    "            total_num = 0\n",
    "            val_loss = 0.0\n",
    "            for j, val in enumerate(test_loader):\n",
    "                val_x, val_label = val\n",
    "                val_x = val_x.to(device)\n",
    "                val_label = val_label.to(device)\n",
    "    \n",
    "                val_output = ecg_net_splitclient(val_x)\n",
    "                val_output = ecg_net_splitserver(val_output)\n",
    "            \n",
    "                val_label = val_label.long()\n",
    "                loss = criterion(val_output, val_label)\n",
    "                val_loss += loss.item()\n",
    "                model_label = val_output.argmax(dim=1)\n",
    "                corr = val_label[val_label == model_label].size(0)\n",
    "                corr_num += corr\n",
    "                total_num += val_label.size(0)\n",
    "            test_accuracy = corr_num / total_num * 100\n",
    "            test_loss = val_loss / len(test_loader)\n",
    "            print(\"test_acc: {:.2f}%, test_loss: {:.4f}\".format(test_accuracy, test_loss))\n",
    "            val_acc.append(test_accuracy)\n",
    "    \n",
    "    print('train is done')\n",
    "    \n",
    "    end_time = time.time()  # store end time\n",
    "    print(\"TrainingTime: {} sec\".format(end_time - start_time))\n",
    "    \n",
    "    # Let's quickly save our trained model:\n",
    "    PATH = './ecg_sp_server.pth'\n",
    "    torch.save(ecg_net_splitserver.state_dict(), PATH)\n",
    "\n",
    "    # if total_weights.isempty()!= False:\n",
    "    if total_weights:\n",
    "        return total_weights\n",
    "\n",
    "    return -1\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d50c19-647d-41bd-aafe-35df2df341bc",
   "metadata": {},
   "source": [
    "# Federated Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffd4e3-b99c-436b-9b7a-6aceee4cddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fac6bbb4-42c1-4b50-a172-ab8665d1443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def average_weights(w, datasize):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "        \n",
    "    for i, data in enumerate(datasize):\n",
    "        for key in w[i].keys():\n",
    "            w[i][key] *= float(data)\n",
    "    \n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    \n",
    "    \n",
    "\n",
    "# when client use only one kinds of device\n",
    "\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(sum(datasize)))\n",
    "\n",
    "# when client use various devices (cpu, gpu) you need to use it instead\n",
    "#\n",
    "#     for key, val in w_avg.items():\n",
    "#         common_device = val.device\n",
    "#         break\n",
    "#     for key in w_avg.keys():\n",
    "#         for i in range(1, len(w)):\n",
    "#             if common_device == 'cpu':\n",
    "#                 w_avg[key] += w[i][key].cpu()\n",
    "#             else:\n",
    "#                 w_avg[key] += w[i][key].cuda()\n",
    "#         w_avg[key] = torch.div(w_avg[key], float(sum(datasize)))\n",
    "\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af262728-68b3-49c9-9fcc-b52ee91f0ded",
   "metadata": {},
   "source": [
    "## Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d90e725-161e-48c2-840f-0bc11f84b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide users into split and federated\n",
    "# clientsoclist_federated = [0] #*users\n",
    "clientsoclist_federated = []\n",
    "clientsoclist_split = [0]  #*users\n",
    "\n",
    "start_time = 0\n",
    "weight_count = 0\n",
    "\n",
    "global_weights = copy.deepcopy(ecg_net_Federated.state_dict())\n",
    "\n",
    "datasetsize = [0]  #*users\n",
    "weights_list = [0]  #*users\n",
    "\n",
    "## if user > 3 then put one user in split user ram = [10,8,4]\n",
    "\n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3469a4b5-fedd-4631-87eb-9c4409f06233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_thread_federated(recieve_federated,train_split,num_user):#,user_ram):\n",
    "    global clientsoclist_federated\n",
    "    global clientsoclist_split\n",
    "    global start_time\n",
    "    \n",
    "    thrs = []\n",
    "    for i in range(num_user):\n",
    "        # if user[i] has ram <4 then break out of loop and send for split learning if not continue\n",
    "        # if user_ram[i] <= 5:\n",
    "        #     pass\n",
    "            \n",
    "        conn, addr = s.accept()\n",
    "        print('Connected with', addr)\n",
    "        # append client socket on list\n",
    "        usercount=1\n",
    "        args1 = (0,usercount,conn,client_weights_split)\n",
    "        args2 = (0, usercount, conn)\n",
    "        ## \n",
    "        if i==1:               #Split\n",
    "            clientsoclist_split[0] = conn  #i\n",
    "            thread = Thread(target=train_split, args=args1)\n",
    "            thrs.append(thread)\n",
    "            thread.start()\n",
    "            print(\"Training Split\")\n",
    "        else:                  #Federated\n",
    "            # clientsoclist_federated[0] = conn  #\n",
    "            clientsoclist_federated.append(conn)\n",
    "            thread = Thread(target=recieve_federated, args=args2)\n",
    "            thrs.append(thread)\n",
    "            thread.start()\n",
    "            print(\"Training Federated\")\n",
    "\n",
    "    # new_weight_list = mergedict(weight_list_federated,total_weights)\n",
    "    # global_weights = average_weights(new_weight_list,datasetsize_federated)\n",
    "                                     \n",
    "    print(\"Timer start!\")\n",
    "    start_time = time.time()    # store start time\n",
    "    for thread in thrs:\n",
    "        thread.join()\n",
    "    end_time = time.time()  # store end time\n",
    "    print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "873db5e3-cee2-4918-9796-e38628f7ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive_federated(userid, num_users, conn): #thread for receive clients\n",
    "    global weight_count\n",
    "    global datasetsize\n",
    "\n",
    "    msg = {\n",
    "        'rounds': rounds,\n",
    "        'client_id': userid,\n",
    "        'local_epoch': local_epoch, # \" Ram \" : user\n",
    "    }\n",
    " \n",
    "    datasize = send_msg(conn, msg)    #send epoch # \n",
    "    total_sendsize_list.append(datasize)\n",
    "    client_sendsize_list[userid].append(datasize)\n",
    "\n",
    "    train_dataset_size, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    "    total_receivesize_list.append(datasize)\n",
    "    client_receivesize_list[userid].append(datasize)\n",
    "    \n",
    "    \n",
    "    with lock:\n",
    "        datasetsize[userid] = train_dataset_size\n",
    "        weight_count += 1\n",
    "    \n",
    "    train_federated(userid, train_dataset_size, num_users, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8256a5bc-c7c5-4a82-90b8-9ddfb6fdb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# global weights_list_federated\n",
    "# global global_weights_federated\n",
    "# global weight_count_federated\n",
    "# global ecg_net_federated\n",
    "# global val_acc_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6a6e8fb-6bad-4e97-a143-f3f2dd95e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_federated(userid, train_dataset_size, num_users, client_conn):\n",
    "    global weights_list\n",
    "    global global_weights\n",
    "    global weight_count\n",
    "    global ecg_net\n",
    "    global val_acc\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        with lock:\n",
    "            if weight_count == num_users:\n",
    "                for i, conn in enumerate(clientsoclist_federated):\n",
    "                    datasize = send_msg(conn, global_weights)\n",
    "                    total_sendsize_list.append(datasize)\n",
    "                    client_sendsize_list[i].append(datasize)\n",
    "                    train_sendsize_list.append(datasize)\n",
    "                    weight_count = 0\n",
    "\n",
    "        client_weights, datasize = recv_msg(client_conn)\n",
    "        total_receivesize_list.append(datasize)\n",
    "        client_receivesize_list[userid].append(datasize)\n",
    "        train_receivesize_list.append(datasize)\n",
    "\n",
    "        weights_list[userid] = client_weights\n",
    "        print(\"User\" + str(userid) + \"'s Round \" + str(r + 1) +  \" is done\")\n",
    "        with lock:\n",
    "            weight_count += 1\n",
    "            if weight_count == num_users:\n",
    "                #average\n",
    "                global_weights = average_weights(weights_list, datasetsize)\n",
    "                \n",
    "# with lock:\n",
    "#     weight_count += 1\n",
    "#     if weight_count == num_users:\n",
    "#         #average\n",
    "#         global_weights = average_weights(weights_list, datasetsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcd94bc5-2b95-4460-a161-3ad9afa7e84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.70.103.230\n"
     ]
    }
   ],
   "source": [
    "host = socket.gethostbyname(socket.gethostname())\n",
    "port = 10081\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef5644c5-7272-413b-ace5-968d85357e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bb142-8415-48fd-a673-9641be6e361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_thread_federated(receive_federated, train_split, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a815d8-de0b-427e-aab9-85825036377c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ad3f3-784b-4661-bd58-45670536a073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065a3d6-c607-44d8-a26d-7158afc12d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "user, user_ram = recv_msg(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad208d-10e9-4481-9af4-9332fd0a1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runmodel(users,user_ram):\n",
    "    federated_users = []\n",
    "    split_users = []\n",
    "    for i in range(len(users)):\n",
    "        if user_ram[i] <= 5:\n",
    "            split_users.append(user[i])\n",
    "        else :\n",
    "            federated_users.append(user[i])\n",
    "    return federated_users,split_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b982e-eec2-495e-8a2d-90a36602c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_users, split_users = runmodel(users,user_ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e114b0-48fb-45a6-bb11-1b7203b659ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsoclist_federated,start_time_federated,weight_count_federated,global_weights_federated,datasetsize_federated,weights_list_federated = hyperparameter_federated(federated_users)                                              \n",
    "criterion_split,lr_split,optimizer_server_split,clientsoclist_split,train_total_batch_split, val_acc_split,client_weights_split,train_acc_split = hyperparameter_split(split_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0a97c-948a-4e8b-b817-a687767c196d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf9cb2-104e-424a-984e-1fec6a9d229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(split_users):\n",
    "#     conn, addr = s.accept()\n",
    "#     print('Connected with', addr)\n",
    "#     clientsoclist.append(conn)    # append client socket on list\n",
    "\n",
    "#     datasize = send_msg(conn, epochs)    #send epoch\n",
    "#     total_sendsize_list.append(datasize)\n",
    "#     client_sendsize_list[i].append(datasize)\n",
    "\n",
    "#     total_batch, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    "#     total_receivesize_list.append(datasize)\n",
    "#     client_receivesize_list[i].append(datasize)\n",
    "\n",
    "#     train_total_batch.append(total_batch)    # append on list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073db6c-12cf-4356-92bd-b0423b59972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_thread_federated(receive, federated_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11285629-21f5-4473-b86f-90f9069846df",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()  # store end time\n",
    "print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb524c0-a712-4025-bf59-59bcb77bd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_net_Federated.load_state_dict(global_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248174e-89fb-4a7f-9c39-2ad478b495a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9158dd-b0f8-4880-8b24-e658b2642857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_split(criterion_split,lr_split,optimizer_server_split,clientsoclist_split,train_total_batch_split, val_acc_split,client_weights_split,train_acc_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e5356a-e6da-48b7-ae61-f5a76f02ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(split_user,federated_user,epochs):\n",
    "#     federated_weights = run_thread_Federated()\n",
    "#     split_weights = train_split()\n",
    "    \n",
    "#     average_weight("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2e507-d545-4d02-b7d3-d96959fc6e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835d6e8-9ef0-42b4-beea-8fd267907577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e9ea8-dac3-4655-92d9-93f72b8efc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f867e5-dece-4697-a3cf-3d2dc2db6638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
