{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Split 1D-CNN Server Side\n",
    "This code is the server part of ECG split 1D-CNN model for **multi** client and a server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "users = 2 # number of users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device ==\"cuda:0\":\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../../models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            with h5py.File(os.path.join(root_path, 'ecg_data', 'train_ecg.hdf5'), 'r') as hdf:\n",
    "                self.x = hdf['x_train'][:]\n",
    "                self.y = hdf['y_train'][:]\n",
    "        else:\n",
    "            with h5py.File(os.path.join(root_path, 'ecg_data', 'test_ecg.hdf5'), 'r') as hdf:\n",
    "                self.x = hdf['x_test'][:]\n",
    "                self.y = hdf['y_test'][:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx], dtype=torch.float), torch.tensor(self.y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make train and test dataset batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ECG(train=True)\n",
    "test_dataset = ECG(train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ECG server model\n",
    "Server side has **1 convolutional layer** and **2 fully connected layers**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcgServer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EcgServer, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(1, 16, 7, padding=3)  # 128 x 16\n",
    "#         self.relu1 = nn.LeakyReLU()\n",
    "#         self.pool1 = nn.MaxPool1d(2)  # 64 x 16\n",
    "#         self.conv2 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "#         self.relu2 = nn.LeakyReLU()\n",
    "        self.conv3 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.conv4 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "        self.relu4 = nn.LeakyReLU()\n",
    "        self.conv5 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "        self.relu5 = nn.LeakyReLU()\n",
    "        self.pool5 = nn.MaxPool1d(2)  # 32 x 16\n",
    "        self.linear6 = nn.Linear(32 * 16, 128)\n",
    "        self.relu6 = nn.LeakyReLU()\n",
    "        self.linear7 = nn.Linear(128, 5)\n",
    "        self.softmax7 = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.pool5(x)\n",
    "        x = x.view(-1, 32 * 16)\n",
    "        x = self.linear6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.linear7(x)\n",
    "        x = self.softmax7(x)\n",
    "        return x       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EcgServer(\n",
      "  (conv3): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu3): LeakyReLU(negative_slope=0.01)\n",
      "  (conv4): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu4): LeakyReLU(negative_slope=0.01)\n",
      "  (conv5): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu5): LeakyReLU(negative_slope=0.01)\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear6): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (relu6): LeakyReLU(negative_slope=0.01)\n",
      "  (linear7): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (softmax7): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ecg_server = EcgServer().to(device)\n",
    "print(ecg_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# print('ECG 1D CNN server')\n",
    "# summary(ecg_server, (16, 58))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECG Client model for calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcgClient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EcgClient, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, 7, padding=3)  # 128 x 16\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)  # 64 x 16\n",
    "        self.conv2 = nn.Conv1d(16, 16, 5, padding=2)  # 64 x 16\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EcgClient(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (relu1): LeakyReLU(negative_slope=0.01)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu2): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ecg_client = EcgClient().to('cpu')\n",
    "print(ecg_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set other hyperparameters in the model\n",
    "Hyperparameters here should be same with the client side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer_server = Adam(ecg_server.parameters(), lr=lr)\n",
    "\n",
    "clientsoclist = []\n",
    "train_total_batch = []\n",
    "val_acc = []\n",
    "client_weights = copy.deepcopy(ecg_client.state_dict())\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sendsize_list = []\n",
    "total_receivesize_list = []\n",
    "\n",
    "client_sendsize_list = [[] for i in range(users)]\n",
    "client_receivesize_list = [[] for i in range(users)]\n",
    "\n",
    "train_sendsize_list = [] \n",
    "train_receivesize_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socket initialization\n",
    "### Set host address and port number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required socket functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msglen\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.83.1\n"
     ]
    }
   ],
   "source": [
    "host = socket.gethostbyname(socket.gethostname())\n",
    "port = 10080\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the server socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conntected with ('192.168.83.1', 1610)\n",
      "Conntected with ('192.168.83.1', 1613)\n"
     ]
    }
   ],
   "source": [
    "for i in range(users):\n",
    "    conn, addr = s.accept()\n",
    "    print('Conntected with', addr)\n",
    "    clientsoclist.append(conn)    # append client socket on list\n",
    "\n",
    "    datasize = send_msg(conn, epochs)    #send epoch\n",
    "    total_sendsize_list.append(datasize)\n",
    "    client_sendsize_list[i].append(datasize)\n",
    "\n",
    "    total_batch, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    "    total_receivesize_list.append(datasize)\n",
    "    client_receivesize_list[i].append(datasize)\n",
    "\n",
    "    train_total_batch.append(total_batch)    # append on list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rlaal\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
      "\r",
      "Epoch 1 Client0 :   0%|                                                     | 0/207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timmer start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Client0 : 100%|███████████████████████████████████████████| 207/207 [00:05<00:00, 35.97it/s]\n",
      "Epoch 1 Client1 : 100%|███████████████████████████████████████████| 207/207 [00:04<00:00, 50.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 80.39%, train_loss: 1.1060\n",
      "test_acc: 81.06%, test_loss: 1.1005\n",
      "train is done\n",
      "TrainingTime: 13.291468620300293 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()    # store start time\n",
    "print(\"timmer start!\")\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train client 0\n",
    "\n",
    "    for user in range(users):\n",
    "\n",
    "        datasize = send_msg(clientsoclist[user], client_weights)\n",
    "        total_sendsize_list.append(datasize)\n",
    "        client_sendsize_list[user].append(datasize)\n",
    "        train_sendsize_list.append(datasize)\n",
    "\n",
    "        for i in tqdm(range(train_total_batch[user]), ncols=100, desc='Epoch {} Client{} '.format(e+1, user)):\n",
    "            optimizer_server.zero_grad()  # initialize all gradients to zero\n",
    "\n",
    "            msg, datasize = recv_msg(clientsoclist[user])  # receive client message from socket\n",
    "            total_receivesize_list.append(datasize)\n",
    "            client_receivesize_list[user].append(datasize)\n",
    "            train_receivesize_list.append(datasize)\n",
    "\n",
    "            client_output_cpu = msg['client_output']  # client output tensor\n",
    "            label = msg['label']  # label\n",
    "\n",
    "            client_output = client_output_cpu.to(device)\n",
    "            label = label.clone().detach().long().to(device)\n",
    "\n",
    "            output = ecg_server(client_output)  # forward propagation\n",
    "            loss = criterion(output, label)  # calculates cross-entropy loss\n",
    "            loss.backward()  # backward propagation\n",
    "            msg = client_output_cpu.grad.clone().detach()\n",
    "\n",
    "            datasize = send_msg(clientsoclist[user], msg)\n",
    "            total_sendsize_list.append(datasize)\n",
    "            client_sendsize_list[user].append(datasize)\n",
    "            train_sendsize_list.append(datasize)\n",
    "            \n",
    "            optimizer_server.step()\n",
    "            \n",
    "        client_weights, datasize = recv_msg(clientsoclist[user])\n",
    "        total_receivesize_list.append(datasize)\n",
    "        client_receivesize_list[user].append(datasize)\n",
    "        train_receivesize_list.append(datasize)\n",
    "        \n",
    "        \n",
    "\n",
    "    ecg_client.load_state_dict(client_weights)\n",
    "    ecg_client.to(device)\n",
    "    ecg_client.eval()\n",
    "\n",
    "\n",
    "    # train acc\n",
    "    with torch.no_grad():\n",
    "        corr_num = 0\n",
    "        total_num = 0\n",
    "        train_loss = 0.0\n",
    "        for j, trn in enumerate(train_loader):\n",
    "            trn_x, trn_label = trn\n",
    "            trn_x = trn_x.to(device)\n",
    "            trn_label = trn_label.to(device)\n",
    "\n",
    "            trn_output = ecg_client(trn_x)\n",
    "            trn_output = ecg_server(trn_output)\n",
    "            trn_label = trn_label.long()\n",
    "            loss = criterion(trn_output, trn_label)\n",
    "            train_loss += loss.item()\n",
    "            model_label = trn_output.argmax(dim=1)\n",
    "            corr = trn_label[trn_label == model_label].size(0)\n",
    "            corr_num += corr\n",
    "            total_num += trn_label.size(0)\n",
    "\n",
    "        train_accuracy = corr_num / total_num * 100\n",
    "        r_train_loss = train_loss / len(train_loader)\n",
    "        print(\"train_acc: {:.2f}%, train_loss: {:.4f}\".format(train_accuracy, r_train_loss))\n",
    "        train_acc.append(train_accuracy)\n",
    "    \n",
    "    # test acc\n",
    "    with torch.no_grad():\n",
    "        corr_num = 0\n",
    "        total_num = 0\n",
    "        val_loss = 0.0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            val_x = val_x.to(device)\n",
    "            val_label = val_label.to(device)\n",
    "\n",
    "            val_output = ecg_client(val_x)\n",
    "            val_output = ecg_server(val_output)\n",
    "        \n",
    "            val_label = val_label.long()\n",
    "            loss = criterion(val_output, val_label)\n",
    "            val_loss += loss.item()\n",
    "            model_label = val_output.argmax(dim=1)\n",
    "            corr = val_label[val_label == model_label].size(0)\n",
    "            corr_num += corr\n",
    "            total_num += val_label.size(0)\n",
    "        test_accuracy = corr_num / total_num * 100\n",
    "        test_loss = val_loss / len(test_loader)\n",
    "        print(\"test_acc: {:.2f}%, test_loss: {:.4f}\".format(test_accuracy, test_loss))\n",
    "        val_acc.append(test_accuracy)\n",
    "\n",
    "print('train is done')\n",
    "\n",
    "end_time = time.time()  # store end time\n",
    "print(\"TrainingTime: {} sec\".format(end_time - start_time))\n",
    "\n",
    "# Let's quickly save our trained model:\n",
    "PATH = './ecg_sp_server.pth'\n",
    "torch.save(ecg_server.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print commmunication overheads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---total_sendsize_list---\n",
      "total_sendsize size: 55273000 bytes\n",
      "\n",
      "\n",
      "---total_receivesize_list---\n",
      "total receive sizes: 55461354 bytes\n",
      "\n",
      "\n",
      "---client_sendsize_list(user0)---\n",
      "total client_sendsizes(user0): 27636500 bytes\n",
      "\n",
      "\n",
      "---client_receivesize_list(user0)---\n",
      "total client_receive sizes(user0): 27730677 bytes\n",
      "\n",
      "\n",
      "---client_sendsize_list(user1)---\n",
      "total client_sendsizes(user1): 27636500 bytes\n",
      "\n",
      "\n",
      "---client_receivesize_list(user1)---\n",
      "total client_receive sizes(user1): 27730677 bytes\n",
      "\n",
      "\n",
      "---train_sendsize_list---\n",
      "total train_sendsizes: 55272990 bytes\n",
      "\n",
      "\n",
      "---train_receivesize_list---\n",
      "total train_receivesizes: 55461344 bytes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('val_acc list')\n",
    "# for acc in val_acc:\n",
    "#     print(acc)\n",
    "\n",
    "print('\\n')\n",
    "print('---total_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in total_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total_sendsize size: {} bytes\".format(total_size))\n",
    "print('\\n')\n",
    "\n",
    "print('---total_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in total_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total receive sizes: {} bytes\".format(total_size) )\n",
    "print('\\n')\n",
    "\n",
    "for i in range(users):\n",
    "    print('---client_sendsize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_sendsize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_sendsizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print('\\n')\n",
    "\n",
    "    print('---client_receivesize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_receivesize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_receive sizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print('\\n')\n",
    "\n",
    "print('---train_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in train_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_sendsizes: {} bytes\".format(total_size))\n",
    "print('\\n')\n",
    "\n",
    "print('---train_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in train_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_receivesizes: {} bytes\".format(total_size))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation after trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 80.39%, train_loss: 1.1060\n"
     ]
    }
   ],
   "source": [
    "# train acc\n",
    "with torch.no_grad():\n",
    "    corr_num = 0\n",
    "    total_num = 0\n",
    "    train_loss = 0.0\n",
    "    for j, trn in enumerate(train_loader):\n",
    "        trn_x, trn_label = trn\n",
    "        trn_x = trn_x.to(device)\n",
    "        trn_label = trn_label.to(device)\n",
    "\n",
    "\n",
    "        trn_output = ecg_client(trn_x)\n",
    "        trn_output = ecg_server(trn_output)\n",
    "        trn_label = trn_label.long()\n",
    "        loss = criterion(trn_output, trn_label)\n",
    "        train_loss += loss.item()\n",
    "        model_label = trn_output.argmax(dim=1)\n",
    "        corr = trn_label[trn_label == model_label].size(0)\n",
    "        corr_num += corr\n",
    "        total_num += trn_label.size(0)\n",
    "    print(\"train_acc: {:.2f}%, train_loss: {:.4f}\".format(corr_num / total_num * 100, train_loss / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 81.06%, test_loss: 1.1005\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    corr_num = 0\n",
    "    total_num = 0\n",
    "    val_loss = 0.0\n",
    "    for j, val in enumerate(test_loader):\n",
    "        val_x, val_label = val\n",
    "        val_x = val_x.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "\n",
    "        val_output = ecg_client(val_x)\n",
    "        val_output = ecg_server(val_output)\n",
    "        \n",
    "        val_label = val_label.long()\n",
    "        loss = criterion(val_output, val_label)\n",
    "        val_loss += loss.item()\n",
    "        model_label = val_output.argmax(dim=1)\n",
    "        corr = val_label[val_label == model_label].size(0)\n",
    "        corr_num += corr\n",
    "        total_num += val_label.size(0)\n",
    "    print(\"test_acc: {:.2f}%, test_loss: {:.4f}\".format(corr_num / total_num * 100, val_loss / len(test_loader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acc of each acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of     N : 75 %\n",
      "Accuracy of     L : 96 %\n",
      "Accuracy of     R : 76 %\n",
      "Accuracy of     A : 42 %\n",
      "Accuracy of     V : 92 %\n",
      "WorkingTime: 25.33627200126648 sec\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(5))\n",
    "class_total = list(0. for i in range(5))\n",
    "classes = ['N', 'L', 'R', 'A', 'V']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        x, labels = data\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = ecg_client(x)\n",
    "        outputs = ecg_server(outputs)\n",
    "        \n",
    "        labels = labels.long()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "\n",
    "# Let's quickly save our trained model:\n",
    "PATH = './ecg_sp_server.pth'\n",
    "torch.save(ecg_server.state_dict(), PATH)\n",
    "PATH = './ecg_sp_client.pth'\n",
    "torch.save(ecg_client.state_dict(), PATH)\n",
    "\n",
    "end_time = time.time()  # store end time\n",
    "print(\"WorkingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
